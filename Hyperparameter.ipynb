{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer \n",
    "\n",
    "def run_stratified_cv(\n",
    "    pipeline: Pipeline,\n",
    "    features: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    n_splits: int = 5,\n",
    "    n_bins: int = 10,\n",
    "    random_state: int = 42,\n",
    "    transform_target: bool = False,\n",
    "    verbose: bool = True\n",
    ") -> tuple[np.ndarray, np.ndarray, float, float, float, float]:\n",
    "    if labels.ndim > 1:\n",
    "      labels = labels.ravel() \n",
    "\n",
    "    logp_bins = pd.cut(labels, bins=n_bins, labels=False, duplicates=\"drop\")\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    offset = 0.0 \n",
    "    if transform_target:\n",
    "        min_label = np.min(labels)\n",
    "        if min_label <= 0:\n",
    "            offset = abs(min_label) + 1 \n",
    "        else:\n",
    "            offset = 1 \n",
    "            \n",
    "    all_rmse, all_r2, all_y_pred, all_y_test = [], [], [], []\n",
    "    if verbose:\n",
    "        print(f\"\\nStarting {n_splits}-Fold CV for:\\n{pipeline}\")\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(features, logp_bins), 1):\n",
    "        \n",
    "        X_train, X_test = features.iloc[train_idx], features.iloc[test_idx]\n",
    "        #X_train, X_test = features[train_idx], features[test_idx]\n",
    "        y_train_orig, y_test_orig = labels[train_idx], labels[test_idx] \n",
    "\n",
    "            \n",
    "        fold_pipeline = clone(pipeline)\n",
    "        if transform_target:\n",
    "            y_train_transformed = np.log(y_train_orig + offset)\n",
    "            target = y_train_transformed\n",
    "        else:\n",
    "            target = y_train_orig\n",
    "\n",
    "        fold_pipeline.fit(X_train, target)\n",
    "        y_pred_raw = fold_pipeline.predict(X_test) \n",
    "        if transform_target:\n",
    "            y_pred_original_scale = np.exp(y_pred_raw) - offset\n",
    "        else:\n",
    "            y_pred_original_scale = y_pred_raw\n",
    "\n",
    "        all_y_pred.append(y_pred_original_scale)\n",
    "        all_y_test.append(y_test_orig)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred_original_scale))\n",
    "        r2 = r2_score(y_test_orig, y_pred_original_scale)\n",
    "        if verbose:\n",
    "            print(f\"Fold {fold} RMSE: {rmse:.4f}, R2 Score: {r2:.4f}\")\n",
    "\n",
    "        all_rmse.append(rmse)\n",
    "        all_r2.append(r2)\n",
    "\n",
    "    final_y_pred = np.concatenate(all_y_pred)\n",
    "    final_y_test = np.concatenate(all_y_test)\n",
    "    mean_rmse = np.nanmean(all_rmse) \n",
    "    std_rmse = np.nanstd(all_rmse)  \n",
    "    mean_r2 = np.nanmean(all_r2)\n",
    "    std_r2 = np.nanstd(all_r2)\n",
    "\n",
    "    print(f\"\\nSummary for: {pipeline}\")\n",
    "    print(f\"Average RMSE: {mean_rmse:.4f} +/- {std_rmse:.4f}\")\n",
    "    print(f\"Average R2 Score: {mean_r2:.4f} +/- {std_r2:.4f}\")\n",
    "    print(\"-------------------------------------------------\") \n",
    "\n",
    "    return final_y_pred, final_y_test, mean_rmse, std_rmse, mean_r2, std_r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"./data/processed/deepchem_mol2vec_300.parquet\")\n",
    "labels = df[\"exp\"].to_numpy()\n",
    "features = df.drop([\"exp\", \"smiles\", \"CMPD_CHEMBLID\"], axis = 1)\n",
    "features = features.to_numpy()\n",
    "labels = labels.reshape(-1, 1)\n",
    "features = np.stack(features.squeeze())\n",
    "\n",
    "morgan_df = pd.read_parquet(\"./data/processed/deepchem_morgan_fp.parquet\")\n",
    "morgan_features = morgan_df.drop([\"exp\"], axis = 1)\n",
    "morgan_features = morgan_features.to_numpy()\n",
    "morgan_features = np.stack(morgan_features.squeeze())\n",
    "\n",
    "extended_df = pd.read_parquet(\"../data/processed/deepchem_extended_mol2vec_300.parquet\")\n",
    "extended_features = extended_df.drop([\"exp\", \"smiles\", \"CMPD_CHEMBLID\"], axis=1)\n",
    "\n",
    "mol2vec_col = extended_df[\"mol2vec\"]\n",
    "extended_descriptors = [\n",
    "    \"molwt\", \"clogp\", \"hba\", \"hbd\",\n",
    "    \"tpsa\",\n",
    "    \"num_rotatable_bonds\",\n",
    "    \"num_rings\",\n",
    "    \"num_aromatic_rings\",\n",
    "    \"fraction_csp3\",\n",
    "    \"num_heavy_atoms\",\n",
    "    \"num_valence_electrons\"\n",
    "]\n",
    "\n",
    "\n",
    "features_mol2vec = np.array(mol2vec_col.tolist(), dtype=np.float64)\n",
    "features_descriptors = extended_df[extended_descriptors].to_numpy(dtype=np.float64)\n",
    "\n",
    "extended_features = np.hstack((features_mol2vec, features_descriptors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "num_features = extended_features.shape[1]\n",
    "generic_feature_names = [f'f{i}' for i in range(num_features)]\n",
    "features_df = pd.DataFrame(extended_features, columns=generic_feature_names)\n",
    "\n",
    "def objective_svr(trial):\n",
    "    svr_params = {\n",
    "        \"C\": trial.suggest_float(\"C\", 1e-1, 1e3, log=True),\n",
    "        \"epsilon\": trial.suggest_float(\"epsilon\", 0.01, 0.5),\n",
    "        \"gamma\": \"scale\",\n",
    "        \"kernel\": \"rbf\"\n",
    "    }\n",
    "    pipeline = make_pipeline(RobustScaler(), SVR(**svr_params))\n",
    "\n",
    "    _, _, mean_rmse, _, _, _ = run_stratified_cv(\n",
    "        pipeline=pipeline,\n",
    "        features=features_df, \n",
    "        labels=labels,\n",
    "        n_splits=5,\n",
    "        verbose=False,\n",
    "        transform_target=False \n",
    "    )\n",
    "    return mean_rmse\n",
    "\n",
    "def objective_lgbm(trial):\n",
    "    lgbm_params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 2500, step=100),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 60),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "        \"verbose\": -1\n",
    "    }\n",
    "    pipeline = make_pipeline(lgb.LGBMRegressor(**lgbm_params))\n",
    "    _, _, mean_rmse, _, _, _ = run_stratified_cv(\n",
    "        pipeline=pipeline,\n",
    "        features=features_df,#extended_features, \n",
    "        labels=labels,\n",
    "        n_splits=5,\n",
    "        verbose=False,\n",
    "        transform_target=False \n",
    "    )\n",
    "    return mean_rmse\n",
    "\n",
    "print(type(extended_features))\n",
    "print(np.shape(extended_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 50 \n",
    "\n",
    "print(f\"\\nStarting LightGBM ({n_trials} trials):\")\n",
    "study_lgbm = optuna.create_study(direction=\"minimize\")\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=n_trials)\n",
    "\n",
    "print(f\"\\nLightGBM Best RMSE: {study_lgbm.best_value:.4f}\")\n",
    "print(\"\\nLightGBM Best hyperparameters:\")\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 50\n",
    "\n",
    "print(f\"\\nStarting SVR ({n_trials} trials):\")\n",
    "study_svr = optuna.create_study(direction=\"minimize\")\n",
    "study_svr.optimize(objective_svr, n_trials=n_trials)\n",
    "\n",
    "print(f\"\\nSVR Best RMSE: {study_svr.best_value:.4f}\")\n",
    "print(\"\\nSVR Best hyperparameters:\")\n",
    "for key, value in study_svr.best_params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
